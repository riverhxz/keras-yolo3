{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
    "#     '''data generator for fit_generator'''\n",
    "#     n = len(annotation_lines)\n",
    "#     i = 0\n",
    "#     while True:\n",
    "#         image_data = []\n",
    "#         box_data = []\n",
    "#         for b in range(batch_size):\n",
    "#             if i == 0:\n",
    "#                 np.random.shuffle(annotation_lines)\n",
    "#             image, box = get_random_data(annotation_lines[i], input_shape, random=True)\n",
    "#             image_data.append(image)\n",
    "#             box_data.append(box)\n",
    "#             i = (i + 1) % n\n",
    "#         image_data = np.array(image_data)\n",
    "#         box_data = np.array(box_data)\n",
    "#         y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes)\n",
    "# #         yield [image_data, *y_true], np.zeros(batch_size)\n",
    "#         yield ( image_data,*y_true)\n",
    "\n",
    "\n",
    "# def data_generator_wrapper(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
    "#     n = len(annotation_lines)\n",
    "#     if n == 0 or batch_size <= 0: return None\n",
    "#     generator =  data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes)\n",
    "\n",
    "#     return tf.data.Dataset.from_generator(\n",
    "#         lambda: generator\n",
    "#         ,output_types= (tf.float32,tf.float32,tf.float32,tf.float32,)\n",
    "#         ,output_shapes=(\n",
    "#             tf.TensorShape(  [None, 416, 416, 3])    \n",
    "#             ,tf.TensorShape( [None, 13, 13, 3, 7])\n",
    "#             ,tf.TensorShape( (None, 26, 26, 3, 7) )\n",
    "#            ,tf.TensorShape(  (None, 52, 52, 3, 7) )\n",
    "#             )\n",
    "#     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def create_model(input_shape, anchors, num_classes, update_callback, load_pretrained=True, freeze_body=2,\n",
    "#                  weights_path='model_data/yolo_weights.h5', input_tensor=None):\n",
    "#     '''create the training model'''\n",
    "#     K.clear_session()  # get a new session\n",
    "#     image_input = input_tensor if input_tensor is not None else Input(shape=(None, None, 3))\n",
    "#     h, w = input_shape\n",
    "#     num_anchors = len(anchors)\n",
    "\n",
    "#     y_true = [Input(shape=(h // {0: 32, 1: 16, 2: 8}[l], w // {0: 32, 1: 16, 2: 8}[l], \\\n",
    "#                            num_anchors // 3, num_classes + 5)) for l in range(3)]\n",
    "\n",
    "#     model_body = yolo_body(image_input, num_anchors // 3, num_classes)\n",
    "#     print('Create YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
    "\n",
    "#     if load_pretrained:\n",
    "#         model_body.load_weights(weights_path, by_name=True)\n",
    "#         print('Load weights {}.'.format(weights_path))\n",
    "#         if freeze_body in [1, 2]:\n",
    "#             # Freeze darknet53 body or freeze all but 3 output layers.\n",
    "#             num = (185, len(model_body.layers) - 3)[freeze_body - 1]\n",
    "#             for i in range(num): model_body.layers[i].trainable = False\n",
    "#             print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
    "\n",
    "#     model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
    "#                         arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.5,\n",
    "#                                    'update_callback': update_callback})(\n",
    "#         [*model_body.output, *y_true])\n",
    "#     model = Model([model_body.input, *y_true], model_loss)\n",
    "\n",
    "#     return model\n",
    "\n",
    "\n",
    "# def model_body(input_shape, anchors, num_classes, update_callback, load_pretrained=True, freeze_body=2,\n",
    "#                  weights_path='model_data/yolo_weights.h5'):\n",
    "#     K.clear_session()  # get a new session\n",
    "#     image_input = Input(shape=(None, None, 3))\n",
    "#     h, w = input_shape\n",
    "#     num_anchors = len(anchors)\n",
    "\n",
    "#     y_true = [Input(shape=(h // {0: 32, 1: 16, 2: 8}[l], w // {0: 32, 1: 16, 2: 8}[l], \\\n",
    "#                            num_anchors // 3, num_classes + 5)) for l in range(3)]\n",
    "\n",
    "#     model_body = yolo_body(image_input, num_anchors // 3, num_classes)\n",
    "#     print('Create YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
    "\n",
    "#     if load_pretrained:\n",
    "#         model_body.load_weights(weights_path, by_name=True)\n",
    "#         print('Load weights {}.'.format(weights_path))\n",
    "#         if freeze_body in [1, 2]:\n",
    "#             # Freeze darknet53 body or freeze all but 3 output layers.\n",
    "#             num = (185, len(model_body.layers) - 3)[freeze_body - 1]\n",
    "#             for i in range(num): model_body.layers[i].trainable = False\n",
    "#             print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
    "\n",
    "    \n",
    "#     return model_body\n",
    "\n",
    "# def loss_wrapper(outputs, pred, anchors, num_classes):\n",
    "#     return yolo_loss([*outputs, *pred], anchors=anchors, num_classes=num_classes, ignore_thresh= 0.5, print_loss=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import get_classes, get_anchors\n",
    "annotation_path = 'data/input.csv'\n",
    "log_dir = 'logs/000/'\n",
    "classes_path = 'model_data/stdogs_classes.txt'\n",
    "anchors_path = 'model_data/yolo_anchors.txt'\n",
    "val_split = 0.5\n",
    "\n",
    "class_names = get_classes(classes_path)\n",
    "num_classes = len(class_names)\n",
    "anchors = get_anchors(anchors_path)\n",
    "\n",
    "input_shape = (416, 416)  # multiple of 32, hw\n",
    "\n",
    "is_tiny_version = len(anchors) == 6  # default setting\n",
    "\n",
    "\n",
    "\n",
    "# model, outputs, y_true = create_model(input_shape, anchors, num_classes, update_callback,\n",
    "#                      freeze_body=2,\n",
    "#                      weights_path='model_data/darknet53.weights.h5')  # make sure you know what you freeze\n",
    "\n",
    "with open(annotation_path) as f:\n",
    "    lines = f.readlines()\n",
    "np.random.seed(10101)\n",
    "np.random.shuffle(lines)\n",
    "np.random.seed(None)\n",
    "num_val = int(len(lines) * val_split)\n",
    "num_train = len(lines) - num_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0521 00:42:34.223285 140736037200768 deprecation.py:323] From /Users/huanghaihun/anaconda3/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:410: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "batch_size = 2\n",
    "train_data = data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes)\n",
    "eval_data = generator = data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for x in data.train_data(2):\n",
    "#     assert x is not None\n",
    "\n",
    "# # lambda: generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create YOLOv3 model with 9 anchors and 2 classes.\n",
      "Load weights model_data/darknet53.weights.h5.\n",
      "Freeze the first 264 layers of total 267 layers.\n"
     ]
    }
   ],
   "source": [
    "# body = model_body(input_shape, anchors, num_classes, None,\n",
    "#                      freeze_body=2,\n",
    "#                      weights_path='model_data/darknet53.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-0f75f5261125>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0myolo3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mignore_thresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0myolo_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnum_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m3\u001b[0m  \u001b[0;31m# default setting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# yolo_outputs = args[:num_layers]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "# from yolo3.model import *\n",
    "# ignore_thresh=0.5\n",
    "# yolo_outputs = outputs\n",
    "# num_layers = len(anchors) // 3  # default setting\n",
    "# # yolo_outputs = args[:num_layers]\n",
    "# y_true = [y1,y2,y3]\n",
    "# anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]] if num_layers == 3 else [[3, 4, 5], [1, 2, 3]]\n",
    "# input_shape = K.cast(K.shape(outputs[0])[1:3] * 32, K.dtype(y_true[0]))\n",
    "# grid_shapes = [K.cast(K.shape(outputs[l])[1:3], K.dtype(y_true[0])) for l in range(num_layers)]\n",
    "# loss = 0\n",
    "# # m = K.shape(yolo_outputs[0])[0]  # batch size, tensor\n",
    "# # mf = K.cast(m, K.dtype(yolo_outputs[0]))\n",
    "# m = batch_size\n",
    "# mf = 1.0 * batch_size\n",
    "# l = 0\n",
    "\n",
    "# object_mask = y_true[l][..., 4:5]\n",
    "# true_class_probs = y_true[l][..., 5:]\n",
    "\n",
    "# grid, raw_pred, pred_xy, pred_wh = yolo_head(yolo_outputs[l],\n",
    "#                                              anchors[anchor_mask[l]], num_classes, input_shape, calc_loss=True)\n",
    "# pred_box = K.concatenate([pred_xy, pred_wh])\n",
    "\n",
    "# # Darknet raw box to calculate loss.\n",
    "# raw_true_xy = y_true[l][..., :2] * grid_shapes[l][::-1] - grid\n",
    "# raw_true_wh = K.log(y_true[l][..., 2:4] / anchors[anchor_mask[l]] * input_shape[::-1])\n",
    "\n",
    "# raw_true_wh = K.switch(object_mask, raw_true_wh, K.zeros_like(raw_true_wh))  # avoid log(0)=-inf\n",
    "# box_loss_scale = 2 - y_true[l][..., 2:3] * y_true[l][..., 3:4]\n",
    "\n",
    "# # Find ignore mask, iterate over each of batch.\n",
    "# ignore_mask = tf.TensorArray(K.dtype(y_true[0]), size=1, dynamic_size=True)\n",
    "# object_mask_bool = K.cast(object_mask, 'bool')\n",
    "# y_true[l][..., 4:5]\n",
    "# def loop_body(b, ignore_mask):\n",
    "#     true_box = tf.boolean_mask(y_true[l][b, ..., 0:4], object_mask_bool[b, ..., 0])\n",
    "#     iou = box_iou(pred_box[b], true_box)\n",
    "#     best_iou = K.max(iou, axis=-1)\n",
    "#     ignore_mask = ignore_mask.write(b, K.cast(best_iou < ignore_thresh, K.dtype(true_box)))\n",
    "\n",
    "#     return b + 1, ignore_mask\n",
    "\n",
    "# from tensorflow.python.ops import control_flow_ops\n",
    "# _, ignore_mask = control_flow_ops.while_loop(lambda b, *args: b < m, loop_body, [0, ignore_mask])\n",
    "# ignore_mask = ignore_mask.stack()\n",
    "# ignore_mask = K.expand_dims(ignore_mask, -1)\n",
    "#        # K.binary_crossentropy is helpful to avoid exp overflow.\n",
    "# xy_loss = object_mask * box_loss_scale * K.binary_crossentropy(raw_true_xy, raw_pred[..., 0:2],\n",
    "#                                                                from_logits=True)\n",
    "# wh_loss = object_mask * box_loss_scale * 0.5 * K.square(raw_true_wh - raw_pred[..., 2:4])\n",
    "# confidence_loss = object_mask * K.binary_crossentropy(object_mask, raw_pred[..., 4:5], from_logits=True) + \\\n",
    "#                   (1 - object_mask) * K.binary_crossentropy(object_mask, raw_pred[..., 4:5],\n",
    "#                                                             from_logits=True) * ignore_mask\n",
    "\n",
    "# class_loss = object_mask * K.binary_crossentropy(true_class_probs, raw_pred[..., 5:7], from_logits=True)\n",
    "\n",
    "# extend_true_class_probs = tf.concat(\n",
    "#     [true_class_probs, 1 - tf.reduce_sum(true_class_probs, axis=4, keepdims=True)], axis=4)\n",
    "\n",
    "# # class_center = tf.Variable(\"class_center\")\n",
    "# multi_mask = 1 - ignore_mask / tf.norm(ignore_mask, 1, keepdims=True)\n",
    "# multi_class_loss = K.squeeze(multi_mask, 4) * tf.nn.softmax_cross_entropy_with_logits(\n",
    "#     labels=extend_true_class_probs\n",
    "#     , logits=raw_pred[..., 7:]\n",
    "# )\n",
    "\n",
    "# xy_loss = K.sum(xy_loss) / mf\n",
    "# wh_loss = K.sum(wh_loss) / mf\n",
    "# confidence_loss = K.sum(confidence_loss) / mf\n",
    "# class_loss = K.sum(class_loss) / mf\n",
    "# multi_class_loss = K.sum(multi_class_loss) / mf\n",
    "# # TODO\n",
    "# loss += (xy_loss + wh_loss + confidence_loss\n",
    "#          + class_loss\n",
    "#          + multi_class_loss\n",
    "#          )\n",
    "\n",
    "# pd.DataFrame(ignore_mask[0,:,:,0,0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6a623f056349>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrue_class_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextend_true_class_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mobject_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_class_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y1' is not defined"
     ]
    }
   ],
   "source": [
    "# true_class_probs = extend_true_class_probs = y1[..., 5:]\n",
    "# object_mask = y1[..., 4:5]\n",
    "# num_pos = tf.reduce_sum(true_class_probs)\n",
    "# tf.reduce_sum(object_mask)\n",
    "# import pandas as pd \n",
    "# pd.DataFrame(tf.reduce_sum(true_class_probs,4)[0,:,:,2].numpy())\n",
    "# pos = tf.reduce_sum(true_class_probs, axis=4, )\n",
    "# all = tf.cast(tf.reduce_prod(tf.shape(true_class_probs)), K.dtype(pos))\n",
    "# weight = 1 - (1 - num_pos / all) * pos\n",
    "# num_pos,pos,all,weight,object_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test extends label\n",
    "# z=tf.Variable(y1[...,5:])\n",
    "\n",
    "def extends(true_class_probs):\n",
    "\n",
    "    extend_true_class_probs = tf.concat(\n",
    "                [true_class_probs, 1 - tf.reduce_sum(true_class_probs, axis=4, keepdims=True)], axis=4)\n",
    "    return tf.argmax(extend_true_class_probs, axis=4)\n",
    "\n",
    "# tf.reduce_sum(extends(z)[:][0] == 2 )\n",
    "# z[...,0].assign(tf.ones_like(z[...,1])) \n",
    "# tf.reduce_sum(extends(z)[:][0] == 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moveing_avg(variable, value, update_weight=0.05):    \n",
    "    return variable.assign(variable * (1-update_weight) + update_weight*value )\n",
    "\n",
    "\n",
    "def update(center, value, keys):\n",
    "    z=extends(keys)\n",
    "    ez = tf.cast(tf.expand_dims(z,4), value[0].dtype)\n",
    "    v_shape = tf.shape(value)\n",
    "    reshape_value = tf.reshape(value,[*v_shape[:-1],1 ,v_shape[-1]])\n",
    "    group_bys = tf.reduce_mean(ez * reshape_value, axis=[0,1,2] )\n",
    "    return moveing_avg(center, group_bys)\n",
    "\n",
    "\n",
    "def update_centers():\n",
    "    gap = outputs[3:6]\n",
    "    keys = [x[..., 5:7] for x in [y1,y2,y3]]\n",
    "    for (center, v,k) in zip(\n",
    "        model.model_global.centers\n",
    "        ,gap\n",
    "        ,keys\n",
    "        ):\n",
    "        update(center, v, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=9692.572265625:  12%|█▎        | 13/104.0 [00:56<06:39,  4.39s/it] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7142e531bead>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mtbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mtbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss={}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-7142e531bead>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(image, y1, y2, y3)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manchors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_classes\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    659\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[0;32m--> 660\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    868\u001b[0m                                 ' implement a `call` method.')\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_internal_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    659\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[0;32m--> 660\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    205\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'NCHW'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'NHWC'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m   2654\u001b[0m       \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2655\u001b[0m       \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bias\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2656\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_nn_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mbias_add\u001b[0;34m(value, bias, data_format, name)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mbias_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"NHWC\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m   r\"\"\"Adds `bias` to `value`.\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import *\n",
    "outputs = None\n",
    "num_epoch=10\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "tf.summary.experimental.set_step(0)\n",
    "val_step = 10\n",
    "def train_step(image,y1,y2,y3):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = body(image)\n",
    "        loss = loss_wrapper(outputs, [y1,y2,y3],anchors,num_classes )\n",
    "        grads = tape.gradient(loss, body.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, body.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "def evaluate_step(image,y1,y2,y3):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = body(image)\n",
    "        loss = loss_wrapper(outputs, [y1,y2,y3],anchors,num_classes )\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    with tqdm(train_data,total=num_train/batch_size) as tbar:\n",
    "        for x in train_data:\n",
    "            image,y1,y2,y3 = x\n",
    "            loss = train_step(image, y1, y2, y3)\n",
    "            tbar.update(1)\n",
    "            tbar.set_description(\"loss={%.3f}\".format(loss))\n",
    "            \n",
    "    \n",
    "    for x in range(val_step):\n",
    "        image,y1,y2,y3 = x\n",
    "        evaluate_step(image, y1, y2, y3)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
